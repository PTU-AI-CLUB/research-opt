{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1516\n",
      "2358\n",
      "943\n",
      "1200\n",
      "2190\n",
      "287\n",
      "2412\n",
      "2428\n",
      "2276\n",
      "1004\n",
      "2987\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from unidecode import unidecode\n",
    "\n",
    "doc = fitz.open(\"test2.pdf\")\n",
    "toc = doc.get_toc(simple=True)\n",
    "\n",
    "for content in toc:\n",
    "    page = content[-1]\n",
    "    title = content[-2]\n",
    "    print(unidecode(doc[page-1].get_text()).find(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Introduction', 1],\n",
       " [1, 'Related Work', 2],\n",
       " [1, 'Framework', 3],\n",
       " [2, 'Unsupervised pre-training', 3],\n",
       " [2, 'Supervised fine-tuning', 3],\n",
       " [2, 'Task-specific input transformations', 4],\n",
       " [1, 'Experiments', 4],\n",
       " [2, 'Setup', 4],\n",
       " [2, 'Supervised fine-tuning', 5],\n",
       " [1, 'Analysis', 7],\n",
       " [1, 'Conclusion', 8]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "The ability to learn effectively from raw text is crucial to alleviating the dependence on supervised\n",
      "learning in natural language processing (NLP). Most deep learning methods require substantial\n",
      "amounts of manually labeled data, which restricts their applicability in many domains that suffer\n",
      "from a dearth of annotated resources [61]. In these situations, models that can leverage linguistic\n",
      "information from unlabeled data provide a valuable alternative to gathering more annotation, which\n",
      "can be time-consuming and expensive. Further, even in cases where considerable supervision\n",
      "is available, learning good representations in an unsupervised fashion can provide a significant\n",
      "performance boost. The most compelling evidence for this so far has been the extensive use of pre-\n",
      "trained word embeddings [10, 39, 42] to improve performance on a range of NLP tasks [8, 11, 26, 45].\n",
      "Leveraging more than word-level information from unlabeled text, however, is challenging for two\n",
      "main reasons. First, it is unclear what type of optimization objectives are most effective at learning\n",
      "text representations that are useful for transfer. Recent research has looked at various objectives\n",
      "such as language modeling [44], machine translation [38], and discourse coherence [22], with each\n",
      "method outperforming the others on different tasks.1 Second, there is no consensus on the most\n",
      "effective way to transfer these learned representations to the target task. Existing techniques involve\n",
      "a combination of making task-specific changes to the model architecture [43, 44], using intricate\n",
      "learning schemes [21] and adding auxiliary learning objectives [50]. These uncertainties have made\n",
      "it difficult to develop effective semi-supervised learning approaches for language processing.\n",
      "1https://gluebenchmark.com/leaderboard\n",
      "Preprint. Work in progress.\n",
      "In this paper, we explore a semi-supervised approach for language understanding tasks using a\n",
      "combination of unsupervised pre-training and supervised fine-tuning. Our goal is to learn a universal\n",
      "representation that transfers with little adaptation to a wide range of tasks. We assume access to\n",
      "a large corpus of unlabeled text and several datasets with manually annotated training examples\n",
      "(target tasks). Our setup does not require these target tasks to be in the same domain as the unlabeled\n",
      "corpus. We employ a two-stage training procedure. First, we use a language modeling objective on\n",
      "the unlabeled data to learn the initial parameters of a neural network model. Subsequently, we adapt\n",
      "these parameters to a target task using the corresponding supervised objective.\n",
      "For our model architecture, we use the Transformer [62], which has been shown to perform strongly on\n",
      "various tasks such as machine translation [62], document generation [34], and syntactic parsing [29].\n",
      "This model choice provides us with a more structured memory for handling long-term dependencies in\n",
      "text, compared to alternatives like recurrent networks, resulting in robust transfer performance across\n",
      "diverse tasks. During transfer, we utilize task-specific input adaptations derived from traversal-style\n",
      "approaches [52], which process structured text input as a single contiguous sequence of tokens. As\n",
      "we demonstrate in our experiments, these adaptations enable us to fine-tune effectively with minimal\n",
      "changes to the architecture of the pre-trained model.\n",
      "We evaluate our approach on four types of language understanding tasks - natural language inference,\n",
      "question answering, semantic similarity, and text classification. Our general task-agnostic model\n",
      "outperforms discriminatively trained models that employ architectures specifically crafted for each\n",
      "task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance,\n",
      "we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40],\n",
      "5.7% on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66] and 5.5% on\n",
      "the recently introduced GLUE multi-task benchmark [64]. We also analyzed zero-shot behaviors\n",
      "of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic\n",
      "knowledge for downstream tasks.\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_start = unidecode(doc[0].get_text()).find(\"Introduction\")\n",
    "idx_end = unidecode(doc[1].get_text()).find(\"Related Work\")\n",
    "\n",
    "content = unidecode(doc[0].get_text())[idx_start:] + unidecode(doc[1].get_text())[:idx_end]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "class DocumentSummarizer:\n",
    "\n",
    "    API_TOKEN = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "\n",
    "    # API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/Falconsai/text_summarization\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self.doc = fitz.open(path)\n",
    "        self.toc = self.doc.get_toc(simple=True)\n",
    "    \n",
    "    def _summarize(self, payload: str):\n",
    "        response = requests.post(self.API_URL, headers=self.headers, json=payload)\n",
    "        return response.json()[0][\"summary_text\"]\n",
    "    \n",
    "    def summarize(self) -> str:\n",
    "        summarized_doc = {}\n",
    "        for i, content in enumerate(self.toc):\n",
    "            title = content[1]\n",
    "            page_no = content[2]\n",
    "\n",
    "            if i+1==len(self.toc):\n",
    "                text = unidecode(self.doc[page_no-1].get_text())\n",
    "                start_idx = text.find(title)\n",
    "                content_text = text[len(title)+start_idx:]\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                start_idx = unidecode(self.doc[page_no-1].get_text()).find(title)\n",
    "                end_idx = unidecode(self.doc[self.toc[i+1][2]-1].get_text()).find(self.toc[i+1][1])\n",
    "                if page_no == self.toc[i+1][2]:\n",
    "                    content_text = unidecode(self.doc[page_no-1].get_text())[len(title)+start_idx:end_idx]\n",
    "                else:\n",
    "                    content_text = unidecode(self.doc[page_no-1].get_text())[len(title)+start_idx:] + \\\n",
    "                                   unidecode(self.doc[self.toc[i+1][2]-1].get_text())[:end_idx]\n",
    "            \n",
    "            content_text = content_text.replace(\"\\n\", \" \")\n",
    "            summzarized_content_text = \"\"\n",
    "            while len(content_text)>512:\n",
    "                summzarized_content_text += self._summarize(content_text[:512])\n",
    "                content_text = content_text[512:]\n",
    "            summarized_doc[title] = summzarized_content_text\n",
    "        \n",
    "        summarized_paper = \"\"\n",
    "        for title, content in summarized_doc.items():\n",
    "            summarized_paper += title + \"\\n\" + content\n",
    "        return summarized_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DocumentSummarizer(path=\"test2.pdf\")\n",
    "res = ds.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "the ability to learn effectively from raw text is crucial to alleviating the dependence on supervised learning in natural language processing (NLP) deep learning methods require substantial amounts of manually labeled data, which restricts their applicability in many domains that suffer from a dearth of annotated resources .the most compelling evidence for this so far has been the extensive use of pre-trained word embeddings [10, 39, 42] to improve performance on a range of NLP tasks [8, 11, 26, 45].optimization objectives are most effective at learning text representations that are useful for transfer . Recent research has looked at various objectives such as language modeling [44], machine translation [38], and discourse coherence [22] .l architecture [43, 44], using intricate learning schemes [21] and adding auxiliary learning objectives [50]. These uncertainties have made it difficult to develop effective semi-supervised learning approaches for language processing tasks . Our goal is to learn a universal representation .We assume access to a large corpus of unlabeled text and several datasets with manually annotated training examples (target tasks) Our setup does not require these target tasks to be in the same domain as the unlabed corpus . We use a two-stage training procedure .Transformer [62] performs strongly on various tasks such as machine translation [62], document generation [34] and syntactic parsing [29]. This model choice provides us with a more structured memory for handling long-term dependencies in text .c input adaptations derived from traversal-style approaches [52] process structured text input as a single contiguous sequence of tokens . We evaluate our approach on four types of language understanding tasks .discriminatively trained models employ architectures specifically crafted for each task . We achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question answering (RACE) [30], 1.5% on textual entailment [66] and 5.5% on the recently introduced GLUE multi-task benchmark [64].Related Work\n",
      "Semi-supervised learning for natural language has attracted significant interest . The earliest approaches used unlabeled data to compute word-level or phrase-level statistics, which were then used as features in a supervised model .eddings are trained on unlabeled corpora to improve performance on a variety of tasks . These approaches transfer word-level information, whereas we aim to capture higher-level semantics . Recent approaches have investigated learning and utilizing more than word .Unsupervised pre-training The goal is to find a good initialization point instead of modifying the supervised learning objective . Early works explored the use of the technique in image classification and regression tasks .the method has been used to train deep neural networks on various tasks . The closest line of work to ours involves pre-training a neural network using a language modeling objective and then fine-tuning it on a target task .our choice of transformer networks allows us to capture longer-range linguistic structure . Other approaches [43, 44, 38] use hidden representations from a 2 pre-trained lt .Auxiliary training objectives Adding auxiliary unsupervised training objectives is an alternative form of semi-supervised learning . Early work by Collobert and Weston [10] used a wide variety of auxiliary NLP tasks .Framework\n",
      "Unsupervised pre-training\n",
      "Un uses a standard language modeling objective to maximize the following likelihood: L1(U) = i log P(ui|ui-k, . , ui-1; Th) (1) where k is the size of the context window, and the conditional probability P is modeled using a neural network with parameters Th . In our experiments, we use a multi-layer Transformer decoder [34] for the language model, which is a variant of the tr .Supervised fine-tuning\n",
      "We perform experiments on a variety of supervised tasks including natural language inference, question answering, semantic similarity, and text classification . Some of these tasks are available as part of the recently released GLUE multi-task benchmark [64] which we make use of . Figure 1 provides an overview of all the tasks and datasets .we evaluate on five datasets with diverse sources, including image captions (SNLI), transcribed speech, popular fiction, and government reports (MNLI), science exams (SciTail) or news arbiter .Table 2 details various results on the different NLI tasks for our model and previous state-of-the-art approaches . Our method significantly outperforms the baselines on four of the five datasets, achieving absolute improvements of upto 1.5% on MNLI, 5% on SciTail, 5.8% on QNLI and 0.6% on SNLI over the previous best results .we achieve an accuracy of 56%, which is below the 61.7% reported by a multi-task biLSTM model . Given the strong performance of our approach on larger NLI datasets, it is likely our model will benefit from multitask training as well .Method Classification Semantic Similarity GLUE CoLA SST2 MRPC STSB QQP (acc) (pc) (f1) Sparse byte mLSTM [16] - 93.2 - - -- TF-KLD [23] . ECNU (mixed ensemble) [60]: - 81.0 - Single-task BiLSTM + ELMo + Attn [64] 35.0 90.2 80.2 55.5 66.1 6Task-specific input transformations\n",
      "previous work proposed learning task specific architectures on top of transferred representations [44] . This model was trained on contiguous sequences of text, such as sentence pairs, or triplets of document, question, and answers .approach re-introduces significant amount of task-specific customization . Instead, we use a traversal-style approach [52] where we convert structured inputs into an ordered sequence that our pre-trained model can process . These input transformations allow us to avoid making extensive changes to the architecture across tasks . Figure 1 provides a visual id .Textual entailment For similarity tasks, there is no inherent ordering of the two sentences being compared . To reflect this, we modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sentences .Question Answering and Commonsense Reasoning For these tasks, we are given a context document z, a question q, and a set of possible answers ak . Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an out of the linear output layer .Experiments\n",
      "Setup\n",
      "Unsupervised pre-training We use the BooksCorpus dataset [71] for training the language model . It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance . An alternative dataset, the 1B Word Benchmark, is approximately the same size 4 Table 1: A list of the differenenen .Task Datasets Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level .del specifications Our model largely follows the original transformer work [62] . We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads).layernorm [2] is used extensively throughout the model . We used a bytepair encoding vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization .We used learned position embeddings instead of sinusoidal version proposed in original work . We use the ftfy library2 to clean raw text in BooksCorpus, standardize punctuation and whitespace .Analysis\n",
      "We observed the impact of transferring a variable number of layers from unsupervised pre-training to the supervised target task . Figure 2 illustrates the performance of our approach on MultiNLI and RACE .the pre-trained language model contains useful functionality for solving target tasks . Figure 2 shows the evolution of zero-shot performance on different tasks as a function of LM pre-training updates . Performance per task is normalized between a random guess baseline and the current state-of-the-art .Method Avg. Score CoLA SST2 MRPC STSB QQP MNLI QNLI RTE (mc) (acc) (f1) (avg) Score is a unweighted average of all the results .Transformer w/ aux LM (full) 74.7 45.4 91.3 82.3 82.0 70.3 81.8 88.1 56.0 . We designed a series of heuristic solutions that use the underlying generative model to perform tasks without supervised finetuning .performance of these heuristics is stable and steadily increases over training . We also observe the LSTM exhibits higher variance in its zero-shot performance suggesting that the inductive bias of the Transformer architecture assists in transfer .examples are scored as the average token log-probability the generative model assigns . For SST-2 (sentiment analysis), we append the token very to each example and restrict the language model's output distribution to only the words positive and negative and guess the token it assigns higher probability to as the prediction .Ablation studies We perform three different ablation studies . First, we examine the performance of our method without the auxiliary LM objective during fine-tuning .We analyze the effect of the Transformer by comparing it with a single layer 2048 unit LSTM using the same framework . We observe a 5.6 average score drop when using the Transformer . Finally, we compare with our transformer architecture directly trained on supervised target tasks .Conclusion\n",
      "generative pre-training and discriminative fine-tuning introduced a framework for achieving strong natural language understanding with a single task-agnostic model . We acquired significant world knowledge and ability to process long-range dependencies which are then successfully transferred to solving discriminative tasks .Using unsupervised (pre-)training to boost performance on discriminative tasks has long been an important goal of Machine Learning research . Our work suggests that achieving significant performance gains is indeed possible . We hope that this will help enable new research into unsupervised learning .\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Introduction', 1],\n",
       " [1, 'Related Work', 2],\n",
       " [1, 'Framework', 3],\n",
       " [2, 'Unsupervised pre-training', 3],\n",
       " [2, 'Supervised fine-tuning', 3],\n",
       " [2, 'Task-specific input transformations', 4],\n",
       " [1, 'Experiments', 4],\n",
       " [2, 'Setup', 4],\n",
       " [2, 'Supervised fine-tuning', 5],\n",
       " [1, 'Analysis', 7],\n",
       " [1, 'Conclusion', 8]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
